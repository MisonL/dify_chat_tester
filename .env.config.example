# ========================================
# AI 聊天客户端测试工具 - 配置模板
# ========================================
# 复制此文件为 .env.config 并根据需要调整参数
# 例如：cp .env.config.example .env.config
# 修改后需要重启程序生效

# === 文件配置 ===
# 对话日志文件名（建议：使用 .xlsx 扩展名）
# 默认: chat_log.xlsx
CHAT_LOG_FILE_NAME=chat_log.xlsx

# === 角色配置 ===
# 可用角色列表（多个角色用英文逗号分隔）
# 提示：也可以在程序运行时直接输入自定义角色名称
# 示例: ROLES=员工,门店,管理员,客服,技术专家,产品经理
ROLES=员工,门店

# === AI 供应商配置 ===
# 支持的 AI 供应商（格式：序号:名称:ID，多个用分号分隔）
# 序号必须是连续的数字，从1开始
# ID 用于程序内部识别，不能重复
# 示例: AI_PROVIDERS=1:Dify:dify;2:OpenAI 兼容接口:openai;3:iFlow:iflow
AI_PROVIDERS=1:Dify:dify;2:OpenAI 兼容接口:openai;3:iFlow:iflow

# === 批量询问配置 ===
# 批量询问时的默认请求间隔时间（秒）
# 建议范围：0.1-5.0
# 较小值：处理更快，但可能触发 API 限制
# 较大值：更安全，适合大规模批量处理
BATCH_REQUEST_INTERVAL=1.0

# 批量询问时是否默认显示回答内容 (true/false)
# true: 批量询问时默认显示每个 AI 的回答
# false: 批量询问时默认不显示回答，只写入 Excel
# 提示：可以在程序运行时覆盖此设置
BATCH_DEFAULT_SHOW_RESPONSE=true

# 批量保存间隔（每处理多少条自动保存一次）
# 较小值：保存更频繁，数据更安全，但 IO 开销更大
# 较大值：保存频率低，性能更好，但中断时可能丢失更多数据
BATCH_SAVE_INTERVAL=10

# === iFlow 模型配置 ===
# iFlow 可用模型列表（多个模型用英文逗号分隔）
# 提示：确保模型名称与 iFlow 平台实际提供的名称完全一致
# 示例: IFLOW_MODELS=qwen3-max,kimi-k2-0905,glm-4.6,deepseek-v3.2,gpt-4o
IFLOW_MODELS=qwen3-max,kimi-k2-0905,glm-4.6,deepseek-v3.2

# === OpenAI 模型配置 ===
# OpenAI 可用模型列表（多个模型用英文逗号分隔）
# 适用于 OpenAI 官方 API 或任何 OpenAI 兼容的服务
# 可以添加任何兼容 OpenAI Chat Completions API 的模型
# 示例: OPENAI_MODELS=gpt-4o,gpt-4o-mini,gpt-4-turbo,gpt-3.5-turbo,custom-model
# 提示：custom-model 是一个占位符，程序会允许用户输入自定义模型名称
OPENAI_MODELS=gpt-4o

# === 等待指示器配置 ===
# 等待动画字符（多个字符用英文逗号分隔）
# 这些字符会依次显示，形成旋转动画效果
# 可以使用任何 Unicode 字符，建议使用 4-8 个字符
# 示例: WAITING_INDICATORS=.,..,... 或 |,/,-,\\
WAITING_INDICATORS=⣾,⣽,⣻,⢿,⡿,⣟,⣯,⣷

# 等待时的提示文字
# 显示在动画字符前面的文字
# 示例: WAITING_TEXT=正在思考 或 Loading 或 处理中
WAITING_TEXT=正在思考

# 动画更新延迟（秒）
# 每次更新动画字符的间隔时间
# 数值越小，动画越快
# 建议范围：0.05-0.5
WAITING_DELAY=0.1

# === 网络重试配置 ===
# 当网络出现超时或连接异常时的自动重试次数（整数）
# 建议范围：1-5，设置为 1 表示不额外重试
NETWORK_MAX_RETRIES=3
# 每次重试之间的等待时间（秒）
NETWORK_RETRY_DELAY=1.0

# === 跨知识点生成配置 ===
# 最少执行多少轮随机组合（整数，至少 1）
CROSS_KNOWLEDGE_MIN_ITERATIONS=5
# 最多执行多少轮随机组合（整数，建议 20-100 之间，过大会增加调用成本）
CROSS_KNOWLEDGE_MAX_ITERATIONS=20

# === 终端 UI 配置 ===
# 是否使用 Rich 提供的彩色面板和表格等富文本 UI
# true: 使用富文本 UI（默认，效果更好，需要终端支持颜色和 Unicode）
# false: 使用简单文本输出（兼容性更好，适合简陋终端或日志环境）
USE_RICH_UI=true

# 是否默认开启思维链/推理过程（仅在模型支持时生效）
# true: 对支持的模型自动请求并展示思维链 (默认)
# false: 始终隐藏思维链，仅展示最终结论
ENABLE_THINKING=true

# === 外部插件配置 ===
# 外部插件目录路径（可选）
# 将自定义插件放在此目录下，程序会自动加载
# 这样可以将扩展插件与核心代码隔离
# 建议填绝对路径，如果填相对路径则是相对于已编译应用/脚本的运行目录
# 示例: EXTERNAL_PLUGINS_PATH=/path/to/my_plugins
# 默认值：external_plugins (在程序运行目录下查找)
EXTERNAL_PLUGINS_PATH=external_plugins

# === 日志配置 ===
# 日志级别：DEBUG / INFO / WARNING / ERROR / CRITICAL（不区分大小写）
# 默认: INFO
LOG_LEVEL=INFO
# 是否写入日志文件（true/false）。为 true 时会在当前目录创建日志文件。
LOG_TO_FILE=false
# 日志文件名（仅在 LOG_TO_FILE=true 时生效）
LOG_FILE_NAME=dify_chat_tester.log
# 日志目录（默认 logs）
LOG_DIR=logs
# 单个日志文件最大字节数（默认 10MB）
LOG_MAX_BYTES=10485760
# 保留的备份文件数量（默认 5）
LOG_BACKUP_COUNT=5

# === API 连接配置与密钥（不推荐在此处设置密钥） ===
# 可以在此处预配置各 Provider 的基础 URL 和 API 密钥，程序会优先读取这些值：
#
# Dify:
# DIFY_BASE_URL=https://api.dify.ai/v1
# DIFY_API_KEY=app-your-dify-api-key-here
# DIFY_APP_ID=your-dify-app-id-here
#
# OpenAI 兼容接口:
# OPENAI_BASE_URL=https://api.openai.com/v1
# OPENAI_API_KEY=sk-your-openai-api-key-here
#
# iFlow:
# IFLOW_API_KEY=sk-your-iflow-api-key-here
#
# 安全提示：将此文件提交到代码仓库会导致密钥泄露，
# 建议仅在本地测试环境使用此方式，生产环境优先使用环境变量或密钥管理服务。

# === AI 提示词配置 ===
# 支持使用三引号包裹多行文本

# 系统提示词（System Prompt）
# 这一段提示词会作为 System Role 发送给 AI
# 支持占位符: {role}
SYSTEM_PROMPT="""
你是一个AI助手。当前角色：{role}。请以专业、友好的方式回答问题。
"""

# 单一知识点生成问题的提示词
# 支持占位符: {idx}, {total_chunks}, {document_name}, {chunk}
SINGLE_KNOWLEDGE_PROMPT="""
你是一个专业的测试问题生成助手。请仔细阅读以下文档内容，生成尽可能多的测试问题。

当前为第 {idx}/{total_chunks} 个内容分块，请尽量覆盖本分块中的知识点。

要求：
1. 先识别本分块中有哪些独立的知识点或主题，并大致评估每个知识点被真实用户询问的概率（高/中/低）。
2. 问题需要模仿普通用户的真实提问语气，口语化、自然。
3. 问题应该覆盖文档中的各个知识点。
4. 问题的难度应该有所变化，包括简单查询、复杂推理等。
5. 每个问题都应该能从文档中找到答案依据。
6. 生成的问题数量不做固定限制：
   - 知识点较少时，覆盖所有知识点即可；
   - 对"高概率"知识点，多写几个不同角度的提问；
   - 对"中概率"知识点，至少生成 1-2 个提问；
   - 对"低概率"知识点，可适当精简。
7. 最终只输出问题列表，不要输出对知识点或概率的解释。
8. 以 JSON 数组格式返回，每个元素是一个问题字符串。

文档名称：{document_name}

文档内容（第 {idx}/{total_chunks} 块）：
{chunk}

请生成问题列表（必须是 JSON 数组格式，例如:["问题1","问题2","问题3"]）：
"""

# 跨知识点生成问题的提示词
# 支持占位符: {context_text}
CROSS_KNOWLEDGE_PROMPT="""
你是一个专业的测试问题生成助手。请阅读以下来自不同（或相同）文档的多个知识点片段，尝试寻找它们之间的关联，生成跨知识点的测试问题。

要求：
1. 先从“真实用户在一个具体场景中”的角度出发，思考这些知识点在实际工作中可能如何被组合使用。
2. 分析提供的多个知识点来源，寻找逻辑关联（例如：同一流程的前后步骤、不同工具在同一任务中的配合、概念/方案的对比、不同角色围绕同一问题的协同等）。
3. 评估这些关联在用户真实场景中出现的概率：
   - 关联度和场景概率较高时，可以多设计几个不同角度的问题；
   - 关联度一般但仍有合理使用场景时，也可以适当生成少量问题；
   - 只有在你几乎无法找到合理关联、强行组合才会误导用户时，才不要生成问题（可以返回空数组）。
4. 每个问题都应该需要结合多个知识点来源的内容才能完整回答（即真正的“跨知识点问题”，不是简单拷贝单一文档的问题）。
5. 问题要模仿真实用户的自然提问，用口语化的方式表达需求。
6. 以 JSON 数组格式返回问题列表；如果最终确实没有合适的问题，再返回空数组 []。

{context_text}

请生成问题列表（必须是 JSON 数组格式，例如: ["问题1", "问题2"]）：
"""

# ========================================
# 快速开始
# ========================================
#
# 1. 复制此文件为 .env.config：
#    cp .env.config.example .env.config
#
# 2. 编辑 .env.config 文件，调整上述参数
#
# 3. 保存文件并重启程序
#
# 4. 验证配置：
#    - 运行程序检查是否有警告信息
#    - 确认所有参数都按预期加载
#
